{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12348a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/cephfs/volumes/hpc_data_usr/k24083007/2070c87e-fe07-4f03-a6c4-cae0de8ce617/cmu-mosei-experiments/CMU-MultimodalSDK-Tutorials\n"
     ]
    }
   ],
   "source": [
    "%cd cmu-mosei-experiments/CMU-MultimodalSDK-Tutorials/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11ec0a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from mmsdk import mmdatasdk as md\n",
    "from constants.paths import DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88f852ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bdfb4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m\u001b[1m[2025-06-30 14:40:27.421] | Success | \u001b[0mComputational sequence read from file ./data/CMU_MOSEI_VisualFacet42.csd ...\n",
      "\u001b[94m\u001b[1m[2025-06-30 14:40:27.494] | Status  | \u001b[0mChecking the integrity of the <FACET 4.2> computational sequence ...\n",
      "\u001b[94m\u001b[1m[2025-06-30 14:40:27.494] | Status  | \u001b[0mChecking the format of the data in <FACET 4.2> computational sequence ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m\u001b[1m[2025-06-30 14:40:28.444] | Success | \u001b[0m<FACET 4.2> computational sequence data in correct format.\n",
      "\u001b[94m\u001b[1m[2025-06-30 14:40:28.444] | Status  | \u001b[0mChecking the format of the metadata in <FACET 4.2> computational sequence ...\n",
      "\u001b[93m\u001b[1m[2025-06-30 14:40:28.444] | Warning | \u001b[0m<FACET 4.2> computational sequence does not have all the required metadata ... continuing \n",
      "\u001b[92m\u001b[1m[2025-06-30 14:40:28.445] | Success | \u001b[0mComputational sequence read from file ./data/CMU_MOSEI_OpenFace2.csd ...\n",
      "\u001b[94m\u001b[1m[2025-06-30 14:40:28.515] | Status  | \u001b[0mChecking the integrity of the <OpenFace_2> computational sequence ...\n",
      "\u001b[94m\u001b[1m[2025-06-30 14:40:28.515] | Status  | \u001b[0mChecking the format of the data in <OpenFace_2> computational sequence ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m\u001b[1m[2025-06-30 14:40:29.437] | Success | \u001b[0m<OpenFace_2> computational sequence data in correct format.\n",
      "\u001b[94m\u001b[1m[2025-06-30 14:40:29.437] | Status  | \u001b[0mChecking the format of the metadata in <OpenFace_2> computational sequence ...\n",
      "\u001b[93m\u001b[1m[2025-06-30 14:40:29.437] | Warning | \u001b[0m<OpenFace_2> computational sequence does not have all the required metadata ... continuing \n",
      "\u001b[92m\u001b[1m[2025-06-30 14:40:29.437] | Success | \u001b[0mDataset initialized successfully ... \n",
      "Both visual feature sets loaded successfully!\n",
      "Available features: ['VisualFacet42', 'OpenFace2']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Recipes for both visual features\n",
    "visual_comparison_recipe = {\n",
    "    'VisualFacet42': os.path.join(DATA_PATH, 'CMU_MOSEI_VisualFacet42.csd'),\n",
    "    'OpenFace2': os.path.join(DATA_PATH, 'CMU_MOSEI_OpenFace2.csd')\n",
    "}\n",
    "\n",
    "# Load both visual feature sets\n",
    "try:\n",
    "    visual_dataset = md.mmdataset(visual_comparison_recipe)\n",
    "    print(\"Both visual feature sets loaded successfully!\")\n",
    "    print(\"Available features:\", list(visual_dataset.keys()))\n",
    "except Exception as e:\n",
    "    print(f\"Error loading visual features: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "861d3a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Dimensionality Analysis\n",
      "============================================================\n",
      "Available fields in visual dataset: ['VisualFacet42', 'OpenFace2']\n",
      "Total segments in dataset: 3837\n",
      "\n",
      "Segment --qXJuDtHPw:\n",
      "  VisualFacet42: (1715, 35) (Time steps x Features)\n",
      "  OpenFace2:     (1714, 713) (Time steps x Features)\n",
      "\n",
      "Feature Dimensions:\n",
      "VisualFacet42: 35 features\n",
      "OpenFace2:     713 features\n"
     ]
    }
   ],
   "source": [
    "# Dimensionality analysis\n",
    "print(\"=\" * 60)\n",
    "print(\"Dimensionality Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"Available fields in visual dataset:\", list(visual_dataset.computational_sequences.keys()))\n",
    "facet42_field = list(visual_dataset.computational_sequences.keys())[0]\n",
    "openface2_field = list(visual_dataset.computational_sequences.keys())[1]\n",
    "\n",
    "# Get the total number of segments\n",
    "total_segments = len(visual_dataset[facet42_field].keys())\n",
    "print(f\"Total segments in dataset: {total_segments}\")\n",
    "\n",
    "# Sample segments are the first 10 visual segments\n",
    "sample_segments = list(visual_dataset[facet42_field].keys())[:10]\n",
    "facet42_dims = []\n",
    "openface2_dims = []\n",
    "\n",
    "# Taking one segment to analyze everything henceforth\n",
    "sample_segment = sample_segments[0]\n",
    "try:\n",
    "    facet42_features = visual_dataset['VisualFacet42'][sample_segment]['features']\n",
    "    openface2_features = visual_dataset['OpenFace2'][sample_segment]['features']\n",
    "    \n",
    "    facet42_dims.append(facet42_features.shape[1])  # Feature dimension\n",
    "    openface2_dims.append(openface2_features.shape[1])\n",
    "    \n",
    "    print(f\"\\nSegment {sample_segment}:\")\n",
    "    print(f\"  VisualFacet42: {facet42_shape} (Time steps x Features)\")\n",
    "    print(f\"  OpenFace2:     {openface2_shape} (Time steps x Features)\")\n",
    "except KeyError:\n",
    "    print(f\"Segment {sample_segment} not found in dataset.\")\n",
    "\n",
    "print(f\"\\nFeature Dimensions:\")\n",
    "print(f\"VisualFacet42: {facet42_dims[0]} features\")\n",
    "print(f\"OpenFace2:     {openface2_dims[0]} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ae9840",
   "metadata": {},
   "source": [
    "CMU SDK has (features, intervals) defined as the arrays\n",
    "\n",
    "Features array shape is (time_steps, 35) for VisualFacet42\n",
    "Intervals array shape is (time_steps, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d944e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Stats Analysis\n",
      "============================================================\n",
      "VisualFacet42 Stats:\n",
      "  Mean: -0.8856\n",
      "  Std:  2.0401\n",
      "  Range: [-18.2228, 16.4741]\n",
      "  Sparsity (% zeros): 0.00%\n",
      "\n",
      "OpenFace2 Stats:\n",
      "  Mean: 166.3502\n",
      "  Std:  451.4024\n",
      "  Range: [-336.6000, 1353.4000]\n",
      "  Sparsity (% zeros): 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Stats analysis\n",
    "print(\"=\" * 60)\n",
    "print(\"Stats Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"VisualFacet42 Stats:\")\n",
    "print(f\"  Mean: {np.mean(facet42_features):.4f}\")\n",
    "print(f\"  Std:  {np.std(facet42_features):.4f}\")\n",
    "print(f\"  Range: [{np.min(facet42_features):.4f}, {np.max(facet42_features):.4f}]\")\n",
    "print(f\"  Sparsity (% zeros): {np.mean(facet42_features == 0) * 100:.2f}%\")\n",
    "\n",
    "print(f\"\\nOpenFace2 Stats:\")\n",
    "print(f\"  Mean: {np.mean(openface2_features):.4f}\")\n",
    "print(f\"  Std:  {np.std(openface2_features):.4f}\")\n",
    "print(f\"  Range: [{np.min(openface2_features):.4f}, {np.max(openface2_features):.4f}]\")\n",
    "print(f\"  Sparsity (% zeros): {np.mean(openface2_features == 0) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b3ffab",
   "metadata": {},
   "source": [
    "OpenFace2 -> larger range, extreme outliers -> necessitating aggressive normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af242f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Temporal Dynamics Analysis\n",
      "============================================================\n",
      "Temporal Variation:\n",
      "  VisualFacet42: 5.8701\n",
      "  OpenFace2:     942.8901\n",
      "\n",
      "Feature Variance Statistics:\n",
      "VisualFacet42:\n",
      "  High variance features (>1.0): 6\n",
      "  Low variance features (<0.1): 4\n",
      "  Mean variance: 1.9168\n",
      "OpenFace2:\n",
      "  High variance features (>1.0): 635\n",
      "  Low variance features (<0.1): 51\n",
      "  Mean variance: 314.0168\n"
     ]
    }
   ],
   "source": [
    "# Temporal Dynamics analysis\n",
    "print(\"=\" * 60)\n",
    "print(\"Temporal Dynamics Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Analysing temporal smoothness\n",
    "def temporal_variation(features):\n",
    "    \"\"\"Calculate frame-to-frame variation to measure temporal smoothness\"\"\"\n",
    "    if len(features) < 2:\n",
    "        return 0\n",
    "    return np.mean(np.sum(np.abs(np.diff(features, axis=0)), axis=1))\n",
    "\n",
    "facet42_temporal_var = temporal_variation(facet42_features)\n",
    "openface2_temporal_var = temporal_variation(openface2_features)\n",
    "\n",
    "print(f\"Temporal Variation:\")\n",
    "print(f\"  VisualFacet42: {facet42_temporal_var:.4f}\")\n",
    "print(f\"  OpenFace2:     {openface2_temporal_var:.4f}\")\n",
    "\n",
    "# Feature variance analysis across time\n",
    "facet42_feature_var = np.var(facet42_features, axis=0)\n",
    "openface2_feature_var = np.var(openface2_features, axis=0)\n",
    "\n",
    "print(f\"\\nFeature Variance Statistics:\")\n",
    "print(f\"VisualFacet42:\")\n",
    "print(f\"  High variance features (>1.0): {np.sum(facet42_feature_var > 1.0)}\")\n",
    "print(f\"  Low variance features (<0.1): {np.sum(facet42_feature_var < 0.1)}\")\n",
    "print(f\"  Mean variance: {np.mean(facet42_feature_var):.4f}\")\n",
    "\n",
    "print(f\"OpenFace2:\")\n",
    "print(f\"  High variance features (>1.0): {np.sum(openface2_feature_var > 1.0)}\")\n",
    "print(f\"  Low variance features (<0.1): {np.sum(openface2_feature_var < 0.1)}\")\n",
    "print(f\"  Mean variance: {np.mean(openface2_feature_var):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f75e12",
   "metadata": {},
   "source": [
    "Lower temporal variation means smoother frame by frame changes\n",
    "\n",
    "VisualFacet42 -> Low variance, \n",
    "OpenFace -> High variance -> noise and over-sensitivity in its high-dimensional outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e37acd0",
   "metadata": {},
   "source": [
    "Choose VisualFacet42 because:\n",
    "- semantically aligned emotion scores, \n",
    "- compact yet rich embeddings, and \n",
    "- smooth temporal behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4117b1b",
   "metadata": {},
   "source": [
    "- High-dimensional OpenFace2 (713 dims) demands PCA, autoencoders, or heavy regularization to avoid overfitting, especially on CMU-MOSEI’s ~4,000 segments\n",
    "\n",
    "- VisualFacet42 was designed for affective analytics, encapsulating key AUs and continuous metrics (valence, engagement) that correlate strongly with emotional states\n",
    "\n",
    "- VisualFacet42’s per‐feature stability and built-in confidence scores (quality flags) enable seamless aleatoric uncertainty modeling via evidential frameworks, unlike OpenFace’s noisy raw landmarks\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jvenv)",
   "language": "python",
   "name": "jvenv"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
