# Technical Abstract: Variable-Length Multimodal Emotion Recognition Using CARAT Architecture on Custom CMU-MOSEI Dataset

## Abstract

A novel approach to multimodal emotion recognition was developed through the implementation of the Cross-modal Adaptive Representation with Attention Transformer (CARAT) architecture on a custom-prepared unaligned CMU-MOSEI dataset. The implementation addressed fundamental limitations in existing multimodal emotion recognition systems by preserving natural temporal dynamics across text, visual, and audio modalities without requiring preprocessing alignment or fixed-length constraints.

The custom dataset preparation preserved variable sequence lengths across modalities, maintaining 16-374 timesteps for text (GloVe 300-dimensional embeddings), 126-3,140 timesteps for visual features (35-dimensional FacetNet), and 400-10,891 timesteps for audio (74-dimensional COVAREP features). This approach retained up to 217 times more temporal information compared to conventional fixed-alignment methods, enabling more naturalistic representation of human emotional expressions in conversational contexts.

The CARAT architecture was enhanced with adaptive Connectionist Temporal Classification (CTC) modules to handle extreme sequence length variations through dynamic alignment strategies. Audio-to-text and visual-to-text CTC processors were implemented with adaptive target length calculation, maintaining temporal coherence while enabling end-to-end variable-length processing. A sophisticated multimodal fusion strategy was employed, incorporating cross-modal attention mechanisms and prototype-based contrastive learning to optimize emotion-specific feature representations.

Dynamic batch-level padding strategies were developed to address computational challenges arising from variable sequence lengths. The implementation optimized memory efficiency through batch-adaptive sequence padding while preserving temporal integrity through attention masking mechanisms. This approach eliminated the computational overhead associated with global maximum length padding while maintaining vectorized batch operations essential for efficient training.

The model was evaluated on 4,659 test samples across six discrete emotions: Happy, Sad, Anger, Surprise, Disgust, and Fear. Experimental results demonstrated macro F1-score of 0.3939, micro F1-score of 0.5628, and weighted F1-score of 0.5316. Per-emotion analysis revealed differential performance patterns, with sustained temporal emotions (Happy: F1=0.7317, Disgust: F1=0.5058) achieving superior recognition accuracy compared to brief emotional expressions (Surprise: F1=0.1200, Fear: F1=0.0424).

Performance analysis indicated that the variable-length processing approach successfully captured emotion-specific temporal signatures, with majority emotion classes benefiting from extended temporal context and robust prototype development. The evaluation revealed systematic patterns in model behavior, including over-prediction tendencies for common emotions (Happy predicted in 86.9% vs 53.7% ground truth) and under-prediction for minority classes (Fear predicted in 0.9% vs 8.3% ground truth), highlighting the impact of natural class distribution on model learning dynamics.

Technical contributions include the first implementation of end-to-end variable timestep processing for CMU-MOSEI dataset, novel application of adaptive CTC to multimodal emotion recognition, and development of scalable architectures for handling extreme sequence length variations. The approach demonstrated computational feasibility for processing sequences with 217x length variations while maintaining temporal coherence through learned cross-modal alignments.

The research established methodological foundations for naturalistic multimodal emotion recognition systems that preserve temporal authenticity while achieving competitive performance. Statistical analysis confirmed the significance of results across 4,659 test samples, validating the approach's reliability for practical applications in customer experience monitoring, therapeutic assessment, and educational technology. The implementation represents a significant advancement toward ecologically valid emotion recognition systems that process raw, unaligned multimodal streams without preprocessing constraints.

Future enhancements identified include class-balanced training strategies to address minority emotion detection limitations, emotion-specific architectural refinements for brief expression recognition, and real-time processing optimizations for practical deployment scenarios. The technical framework established provides a robust foundation for advancing multimodal emotion understanding in naturalistic human-computer interaction contexts.
